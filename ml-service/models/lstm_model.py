import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint
import os
from config import Config
import joblib
from datetime import datetime

class LSTMForecastModel:
    """LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è PM2.5"""
    
    def __init__(self, district_id):
        self.district_id = district_id
        self.model = None
        self.model_path = os.path.join(Config.MODEL_PATH, f'lstm_model_district_{district_id}.keras')
        self.history_path = os.path.join(Config.MODEL_PATH, f'training_history_{district_id}.pkl')
        self.sequence_length = Config.SEQUENCE_LENGTH
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π
        os.makedirs(Config.MODEL_PATH, exist_ok=True)
    
    def build_model(self, input_shape):
        """
        –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É LSTM –º–æ–¥–µ–ª—ñ
        
        Args:
            input_shape: (sequence_length, n_features)
        """
        model = keras.Sequential([
            # –ü–µ—Ä—à–∏–π LSTM —à–∞—Ä
            layers.LSTM(
                units=64,
                return_sequences=True,
                input_shape=input_shape,
                name='lstm_1'
            ),
            layers.Dropout(0.2, name='dropout_1'),
            
            # –î—Ä—É–≥–∏–π LSTM —à–∞—Ä
            layers.LSTM(
                units=32,
                return_sequences=False,
                name='lstm_2'
            ),
            layers.Dropout(0.2, name='dropout_2'),
            
            # Dense —à–∞—Ä–∏
            layers.Dense(16, activation='relu', name='dense_1'),
            layers.Dropout(0.1, name='dropout_3'),
            
            # –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä (–ø—Ä–æ–≥–Ω–æ–∑ PM2.5)
            layers.Dense(1, activation='linear', name='output')
        ])
        
        # –ö–æ–º–ø—ñ–ª—è—Ü—ñ—è –º–æ–¥–µ–ª—ñ
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mean_squared_error',
            metrics=['mae', 'mse']
        )
        
        self.model = model
        
        print("‚úÖ LSTM –º–æ–¥–µ–ª—å —Å—Ç–≤–æ—Ä–µ–Ω–æ")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {model.count_params():,}")
        
        return model
    
    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=50, batch_size=32):
        """
        –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å
        
        Args:
            X_train: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ (sequences)
            y_train: –¶—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            X_val: –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            y_val: –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ —Ü—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            epochs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
            batch_size: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞
        
        Returns:
            History –æ–±'—î–∫—Ç
        """
        if self.model is None:
            self.build_model(input_shape=(X_train.shape[1], X_train.shape[2]))
        
        # Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_loss' if X_val is not None else 'loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                self.model_path,
                monitor='val_loss' if X_val is not None else 'loss',
                save_best_only=True,
                verbose=0
            )
        ]
        
        # –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ
        validation_data = (X_val, y_val) if X_val is not None and y_val is not None else None
        
        print(f"\nüéØ –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è —Ä–∞–π–æ–Ω—É {self.district_id}")
        print(f"   –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤: {len(X_train)}")
        if validation_data:
            print(f"   –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤: {len(X_val)}")
        print(f"   Epochs: {epochs}, Batch size: {batch_size}")
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        history = self.model.fit(
            X_train, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        history_dict = {
            'loss': history.history['loss'],
            'mae': history.history['mae'],
            'mse': history.history['mse'],
            'trained_at': datetime.now().isoformat(),
            'epochs': len(history.history['loss']),
            'district_id': self.district_id
        }
        
        if validation_data:
            history_dict['val_loss'] = history.history['val_loss']
            history_dict['val_mae'] = history.history['val_mae']
            history_dict['val_mse'] = history.history['val_mse']
        
        joblib.dump(history_dict, self.history_path)
        
        print(f"\n‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   –§—ñ–Ω–∞–ª—å–Ω–∏–π loss: {history.history['loss'][-1]:.4f}")
        print(f"   –§—ñ–Ω–∞–ª—å–Ω–∏–π MAE: {history.history['mae'][-1]:.4f}")
        print(f"   –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {self.model_path}")
        
        return history
    
    def load_model(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—É –º–æ–¥–µ–ª—å"""
        if os.path.exists(self.model_path):
            self.model = keras.models.load_model(self.model_path)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {self.model_path}")
            return True
        else:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {self.model_path}")
            return False
    
    def predict(self, X):
        """
        –ó—Ä–æ–±–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑
        
        Args:
            X: –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ (sequences)
        
        Returns:
            –ü—Ä–æ–≥–Ω–æ–∑–∏
        """
        if self.model is None:
            if not self.load_model():
                raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –°–ø–æ—á–∞—Ç–∫—É –ø–æ—Ç—Ä—ñ–±–Ω–æ –Ω–∞—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å.")
        
        predictions = self.model.predict(X, verbose=0)
        return predictions.flatten()
    
    def predict_future(self, last_sequence, n_hours=24):
        """
        –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ N –≥–æ–¥–∏–Ω –≤–ø–µ—Ä–µ–¥
        
        Args:
            last_sequence: –û—Å—Ç–∞–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å (sequence_length, n_features)
            n_hours: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≥–æ–¥–∏–Ω –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É
        
        Returns:
            –ú–∞—Å–∏–≤ –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤
        """
        if self.model is None:
            if not self.load_model():
                raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        
        predictions = []
        current_sequence = last_sequence.copy()
        
        for _ in range(n_hours):
            # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó –≥–æ–¥–∏–Ω–∏
            next_pred = self.model.predict(current_sequence.reshape(1, *current_sequence.shape), verbose=0)[0, 0]
            predictions.append(next_pred)
            
            # –û–Ω–æ–≤–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å (rolling window)
            # –ó—Å—É–≤–∞—î–º–æ –Ω–∞ 1 –ø–æ–∑–∏—Ü—ñ—é —ñ –¥–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π –ø—Ä–æ–≥–Ω–æ–∑
            current_sequence = np.roll(current_sequence, -1, axis=0)
            current_sequence[-1, 0] = next_pred  # PM2.5 –≤ –ø–µ—Ä—à—ñ–π –∫–æ–ª–æ–Ω—Ü—ñ
            
            # –Ü–Ω—à—ñ –æ–∑–Ω–∞–∫–∏ (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –≤–æ–ª–æ–≥—ñ—Å—Ç—å —ñ —Ç.–¥.) –±–µ—Ä–µ–º–æ –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è
            # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç—É—Ç –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥–∏
        
        return np.array(predictions)
    
    def evaluate(self, X_test, y_test):
        """
        –û—Ü—ñ–Ω–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        
        Args:
            X_test: –¢–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ
            y_test: –¢–µ—Å—Ç–æ–≤—ñ —Ü—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        
        Returns:
            Dict –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        if self.model is None:
            if not self.load_model():
                raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        
        results = self.model.evaluate(X_test, y_test, verbose=0)
        
        metrics = {
            'loss': results[0],
            'mae': results[1],
            'mse': results[2],
            'rmse': np.sqrt(results[2])
        }
        
        print(f"\nüìä –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ:")
        print(f"   Loss: {metrics['loss']:.4f}")
        print(f"   MAE: {metrics['mae']:.4f}")
        print(f"   RMSE: {metrics['rmse']:.4f}")
        
        return metrics
    
    def get_model_info(self):
        """–û—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –º–æ–¥–µ–ª—å"""
        info = {
            'district_id': self.district_id,
            'model_exists': os.path.exists(self.model_path),
            'model_path': self.model_path,
            'sequence_length': self.sequence_length
        }
        
        if os.path.exists(self.history_path):
            history = joblib.load(self.history_path)
            info['last_trained'] = history.get('trained_at')
            info['epochs_trained'] = history.get('epochs')
            info['final_loss'] = history['loss'][-1] if history.get('loss') else None
        
        return info