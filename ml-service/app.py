# ml-service/app.py
from flask import Flask, jsonify, request
from flask_cors import CORS
from config import Config
from utils.db_helper import DatabaseHelper
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import traceback

app = Flask(__name__)
CORS(app)

db = DatabaseHelper()

# ==================== HELPER FUNCTIONS ====================

def linear_interpolation(value, c_low, c_high, aqi_low, aqi_high):
    """–õ—ñ–Ω—ñ–π–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—è –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É AQI"""
    return ((aqi_high - aqi_low) / (c_high - c_low)) * (value - c_low) + aqi_low

def calculate_aqi_from_pm25(pm25):
    """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI –∑ PM2.5 (Œºg/m¬≥)"""
    if pm25 <= 12.0:
        return linear_interpolation(pm25, 0, 12.0, 0, 50)
    elif pm25 <= 35.4:
        return linear_interpolation(pm25, 12.1, 35.4, 51, 100)
    elif pm25 <= 55.4:
        return linear_interpolation(pm25, 35.5, 55.4, 101, 150)
    elif pm25 <= 150.4:
        return linear_interpolation(pm25, 55.5, 150.4, 151, 200)
    elif pm25 <= 250.4:
        return linear_interpolation(pm25, 150.5, 250.4, 201, 300)
    else:
        return linear_interpolation(pm25, 250.5, 500.4, 301, 500)

def calculate_aqi_from_pm10(pm10):
    """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI –∑ PM10 (Œºg/m¬≥)"""
    if pm10 <= 54:
        return linear_interpolation(pm10, 0, 54, 0, 50)
    elif pm10 <= 154:
        return linear_interpolation(pm10, 55, 154, 51, 100)
    elif pm10 <= 254:
        return linear_interpolation(pm10, 155, 254, 101, 150)
    elif pm10 <= 354:
        return linear_interpolation(pm10, 255, 354, 151, 200)
    elif pm10 <= 424:
        return linear_interpolation(pm10, 355, 424, 201, 300)
    else:
        return linear_interpolation(pm10, 425, 604, 301, 500)

def calculate_aqi_from_no2(no2):
    """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI –∑ NO2 (Œºg/m¬≥)"""
    no2_ppb = no2 / 1.88
    
    if no2_ppb <= 53:
        return linear_interpolation(no2_ppb, 0, 53, 0, 50)
    elif no2_ppb <= 100:
        return linear_interpolation(no2_ppb, 54, 100, 51, 100)
    elif no2_ppb <= 360:
        return linear_interpolation(no2_ppb, 101, 360, 101, 150)
    elif no2_ppb <= 649:
        return linear_interpolation(no2_ppb, 361, 649, 151, 200)
    elif no2_ppb <= 1249:
        return linear_interpolation(no2_ppb, 650, 1249, 201, 300)
    else:
        return linear_interpolation(no2_ppb, 1250, 2049, 301, 500)

def calculate_aqi_from_so2(so2):
    """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI –∑ SO2 (Œºg/m¬≥)"""
    so2_ppb = so2 / 2.62
    
    if so2_ppb <= 35:
        return linear_interpolation(so2_ppb, 0, 35, 0, 50)
    elif so2_ppb <= 75:
        return linear_interpolation(so2_ppb, 36, 75, 51, 100)
    elif so2_ppb <= 185:
        return linear_interpolation(so2_ppb, 76, 185, 101, 150)
    elif so2_ppb <= 304:
        return linear_interpolation(so2_ppb, 186, 304, 151, 200)
    elif so2_ppb <= 604:
        return linear_interpolation(so2_ppb, 305, 604, 201, 300)
    else:
        return linear_interpolation(so2_ppb, 605, 1004, 301, 500)

def calculate_aqi_from_co(co):
    """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI –∑ CO (Œºg/m¬≥)"""
    co_ppm = co / 1150
    
    if co_ppm <= 4.4:
        return linear_interpolation(co_ppm, 0, 4.4, 0, 50)
    elif co_ppm <= 9.4:
        return linear_interpolation(co_ppm, 4.5, 9.4, 51, 100)
    elif co_ppm <= 12.4:
        return linear_interpolation(co_ppm, 9.5, 12.4, 101, 150)
    elif co_ppm <= 15.4:
        return linear_interpolation(co_ppm, 12.5, 15.4, 151, 200)
    elif co_ppm <= 30.4:
        return linear_interpolation(co_ppm, 15.5, 30.4, 201, 300)
    else:
        return linear_interpolation(co_ppm, 30.5, 50.4, 301, 500)

def calculate_aqi_from_o3(o3):
    """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI –∑ O3 (Œºg/m¬≥)"""
    o3_ppb = o3 / 2.0
    
    if o3_ppb <= 54:
        return linear_interpolation(o3_ppb, 0, 54, 0, 50)
    elif o3_ppb <= 70:
        return linear_interpolation(o3_ppb, 55, 70, 51, 100)
    elif o3_ppb <= 85:
        return linear_interpolation(o3_ppb, 71, 85, 101, 150)
    elif o3_ppb <= 105:
        return linear_interpolation(o3_ppb, 86, 105, 151, 200)
    elif o3_ppb <= 200:
        return linear_interpolation(o3_ppb, 106, 200, 201, 300)
    else:
        return 301

def calculate_overall_aqi(pm25, pm10, no2, so2, co, o3):
    """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –∑–∞–≥–∞–ª—å–Ω–∏–π AQI —è–∫ –º–∞–∫—Å–∏–º—É–º –∑ —É—Å—ñ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
    aqis = {
        'pm25': calculate_aqi_from_pm25(pm25),
        'pm10': calculate_aqi_from_pm10(pm10),
        'no2': calculate_aqi_from_no2(no2),
        'so2': calculate_aqi_from_so2(so2),
        'co': calculate_aqi_from_co(co),
        'o3': calculate_aqi_from_o3(o3)
    }
    
    max_aqi = max(aqis.values())
    dominant = max(aqis, key=aqis.get)
    
    return int(max_aqi), dominant, aqis

def get_aqi_status(aqi):
    """–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–∞—Ç—É—Å —è–∫–æ—Å—Ç—ñ –ø–æ–≤—ñ—Ç—Ä—è"""
    if aqi <= 50: return 'Good'
    elif aqi <= 100: return 'Moderate'
    elif aqi <= 150: return 'Unhealthy for Sensitive'
    elif aqi <= 200: return 'Unhealthy'
    elif aqi <= 300: return 'Very Unhealthy'
    else: return 'Hazardous'

# ==================== API ENDPOINTS ====================

@app.route('/health', methods=['GET'])
def health_check():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è —Å–µ—Ä–≤—ñ—Å—É"""
    return jsonify({
        'status': 'healthy',
        'service': 'ml-service',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/predict/<int:district_id>', methods=['GET'])
def predict_district(district_id):
    """–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä–∞–π–æ–Ω—É"""
    try:
        if district_id < 1 or district_id > 6:
            return jsonify({'success': False, 'error': 'Invalid district_id'}), 400
        
        hours = request.args.get('hours', default=24, type=int)
        if hours not in [12, 24, 48]:
            hours = 24
        
        print(f"\nüîÆ –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Ä–∞–π–æ–Ω—É {district_id} –Ω–∞ {hours} –≥–æ–¥–∏–Ω...")
        
        df = db.get_training_data(district_id, days=2)
        
        if len(df) < 10:
            return jsonify({
                'success': False,
                'error': f'Not enough historical data: {len(df)} records'
            }), 400
        
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
        
        from models.simple_forecast_model import SimpleForecastModel
        
        simple_model = SimpleForecastModel(district_id)
        recent_data = df[['pm25', 'pm10', 'no2', 'so2', 'co', 'o3']].tail(24)
        
        if len(recent_data) < 5:
            return jsonify({
                'success': False,
                'error': 'Not enough recent data for forecast'
            }), 400
        
        print(f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–æ–≥–Ω–æ–∑—É –Ω–∞ {hours} –≥–æ–¥–∏–Ω...")
        forecast_df = simple_model.predict(recent_data, hours=hours)
        
        forecasts = []
        last_time = df['measured_at'].max()
        
        for i, row in forecast_df.iterrows():
            forecast_time = last_time + timedelta(hours=i+1)
            
            aqi, dominant, aqi_breakdown = calculate_overall_aqi(
                row['pm25'], row['pm10'], row['no2'],
                row['so2'], row['co'], row['o3']
            )
            
            forecasts.append({
                'measured_at': forecast_time.isoformat(),
                'pm25': round(float(row['pm25']), 2),
                'pm10': round(float(row['pm10']), 2),
                'no2': round(float(row['no2']), 2),
                'so2': round(float(row['so2']), 2),
                'co': round(float(row['co']), 2),
                'o3': round(float(row['o3']), 2),
                'aqi': aqi,
                'aqi_status': get_aqi_status(aqi),
                'dominant_pollutant': dominant
            })
        
        print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ {len(forecasts)} –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤")
        
        forecasts_df = pd.DataFrame(forecasts)
        db.save_forecasts(district_id, forecasts_df)
        
        return jsonify({
            'success': True,
            'district_id': district_id,
            'hours': hours,
            'model_type': 'persistence_trend',
            'forecasts': forecasts
        })
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {str(e)}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/predict/all', methods=['GET'])
def predict_all_districts():
    """–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –≤—Å—ñ—Ö —Ä–∞–π–æ–Ω—ñ–≤"""
    try:
        hours = request.args.get('hours', default=24, type=int)
        results = []
        
        for district in Config.DISTRICTS:
            try:
                response = predict_district(district['id'])
                data = response.get_json()
                
                if data.get('success'):
                    results.append({
                        'district_id': district['id'],
                        'district_name': district['name'],
                        'success': True,
                        'forecasts_count': len(data['forecasts'])
                    })
                else:
                    results.append({
                        'district_id': district['id'],
                        'district_name': district['name'],
                        'success': False,
                        'error': data.get('error')
                    })
            except Exception as e:
                results.append({
                    'district_id': district['id'],
                    'district_name': district['name'],
                    'success': False,
                    'error': str(e)
                })
        
        return jsonify({'success': True, 'results': results})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/model/<int:district_id>/info', methods=['GET'])
def get_model_info(district_id):
    """–û—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –º–æ–¥–µ–ª—å"""
    try:
        stats = db.get_data_stats(district_id)
        return jsonify({
            'success': True,
            'district_id': district_id,
            'training_data': stats
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/model/<int:district_id>/monitor', methods=['GET'])
def monitor_model(district_id):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —è–∫—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ"""
    from utils.model_monitor import ModelMonitor
    monitor = ModelMonitor()
    result = monitor.auto_retrain_if_needed(district_id)
    return jsonify({'success': True, 'result': result})

@app.route('/api/model/<int:district_id>/retrain', methods=['POST'])
def force_retrain(district_id):
    """–ü—Ä–∏–º—É—Å–æ–≤–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å"""
    from utils.model_monitor import ModelMonitor
    monitor = ModelMonitor()
    result = monitor.retrain_model(district_id)
    return jsonify(result)

@app.route('/api/monitor/all', methods=['POST'])
def monitor_all_districts():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Ç–∞ –ø–µ—Ä–µ–Ω–∞–≤—á–∏—Ç–∏ –≤—Å—ñ –º–æ–¥–µ–ª—ñ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ"""
    from utils.model_monitor import ModelMonitor
    monitor = ModelMonitor()
    results = []
    
    for district in Config.DISTRICTS:
        result = monitor.auto_retrain_if_needed(district['id'])
        results.append(result)
    
    return jsonify({'success': True, 'results': results})

@app.route('/test-model', methods=['POST'])
def test_model():
    """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è ML –º–æ–¥–µ–ª—ñ –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—é –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–æ—é –Ω–∞ data leakage"""
    try:
        data = request.json
        district_id = data.get('district_id')
        days = data.get('days', 30)
        
        print(f"\n{'='*70}")
        print(f"üß™ TIME SERIES –¢–ï–°–¢–£–í–ê–ù–ù–Ø - –†–∞–π–æ–Ω {district_id}")
        print(f"{'='*70}")
        
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
        print(f"\n1Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑–∞ {days} –¥–Ω—ñ–≤...")
        query = """
            SELECT aqi, pm25, pm10, no2, so2, co, o3,
                   temperature, humidity, pressure, 
                   wind_speed, wind_direction, measured_at
            FROM air_quality_history
            WHERE district_id = %s AND is_forecast = false
                AND measured_at >= NOW() - INTERVAL '%s days'
            ORDER BY measured_at
        """
        
        conn = db.get_connection()
        df = pd.read_sql_query(query, conn, params=(district_id, days))
        conn.close()
        
        if len(df) < 100:
            return jsonify({
                'success': False,
                'error': f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: {len(df)} –∑–∞–ø–∏—Å—ñ–≤ (–ø–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º 100)'
            }), 400
        
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
        
        # 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ features
        print("\n2Ô∏è‚É£ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ features...")
        from data.preprocessor import DataPreprocessor
        preprocessor = DataPreprocessor(district_id)
        
        df_processed = preprocessor.prepare_features(df)
        print(f"‚úÖ Features –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {df_processed.shape}")
        
        # 3. Time Series Split –∑ GAP
        print("\n3Ô∏è‚É£ Time Series Split –∑ gap (—É–Ω–∏–∫–∞—î–º–æ data leakage)...")
        
        parameters = ['pm25', 'pm10', 'no2', 'so2', 'co', 'o3']
        feature_cols = preprocessor.get_feature_columns()
        
        # Train: 70%, Gap: 5%, Test: 25%
        total_size = len(df_processed)
        train_size = int(total_size * 0.70)
        gap_size = int(total_size * 0.05)
        test_start = train_size + gap_size
        
        print(f"   Train: 0-{train_size} ({train_size} –∑–∞–ø–∏—Å—ñ–≤)")
        print(f"   Gap: {train_size}-{test_start} ({gap_size} –∑–∞–ø–∏—Å—ñ–≤) ‚Üê –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è!")
        print(f"   Test: {test_start}-{total_size} ({total_size - test_start} –∑–∞–ø–∏—Å—ñ–≤)")
        
        # –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ
        train_df = df_processed.iloc[:train_size].copy()
        test_df = df_processed.iloc[test_start:].copy()
        
        # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ X, y
        X_train = train_df[feature_cols].values
        y_train = train_df[parameters].values
        
        X_test = test_df[feature_cols].values
        y_test = test_df[parameters].values
        
        # 4. –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (scaler –¢–Ü–õ–¨–ö–ò –Ω–∞ train)
        print("\n4Ô∏è‚É£ –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (scaler –Ω–∞ train)...")
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        print(f"‚úÖ Train scaled: {X_train_scaled.shape}")
        print(f"‚úÖ Test scaled: {X_test_scaled.shape}")
        
        # 5. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        print("\n5Ô∏è‚É£ –ù–∞–≤—á–∞–Ω–Ω—è XGBoost –º–æ–¥–µ–ª—ñ...")
        from models.air_quality_model import AirQualityModel
        
        model = AirQualityModel(district_id, model_type='xgboost')
        train_score, val_score = model.train(X_train_scaled, y_train, X_test_scaled, y_test)
        
        print(f"‚úÖ Train R¬≤: {train_score:.4f}, Test R¬≤: {val_score:.4f}")
        
        # 6. –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ
        print("\n6Ô∏è‚É£ –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ...")
        predictions = model.predict(X_test_scaled)
        
        # 7. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫
        print("\n7Ô∏è‚É£ –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫...")
        
        metrics = {}
        
        for i, param in enumerate(parameters):
            y_true = y_test[:, i]
            y_pred = predictions[:, i]
            
            mae = float(np.mean(np.abs(y_true - y_pred)))
            rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
            
            ss_res = np.sum((y_true - y_pred) ** 2)
            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
            r2 = float(1 - (ss_res / ss_tot)) if ss_tot > 0 else 0
            
            mask = y_true != 0
            mape = float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100) if mask.any() else 0
            
            threshold = np.maximum(y_true * 0.1, 5)
            accurate = np.abs(y_true - y_pred) <= threshold
            accuracy = float(np.mean(accurate) * 100)
            
            metrics[param] = {
                'mae': round(mae, 2),
                'rmse': round(rmse, 2),
                'r2': round(r2, 4),
                'mape': round(mape, 2),
                'accuracy': round(accuracy, 2),
                'avgActual': round(float(np.mean(y_true)), 2),
                'avgPredicted': round(float(np.mean(y_pred)), 2),
                'samples': len(y_true)
            }
            
            print(f"   {param.upper()}: MAE={mae:.2f}, RMSE={rmse:.2f}, R¬≤={r2:.4f}, Accuracy={accuracy:.1f}%")
        
        # 8. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI
        print("\n8Ô∏è‚É£ –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ AQI...")
        
        aqi_actual = []
        aqi_predicted = []
        
        for idx in range(len(y_test)):
            pm25_actual = y_test[idx, 0]
            aqi_actual.append(calculate_aqi_from_pm25(pm25_actual))
            
            pm25_pred = predictions[idx, 0]
            aqi_predicted.append(calculate_aqi_from_pm25(pm25_pred))
        
        aqi_actual = np.array(aqi_actual)
        aqi_predicted = np.array(aqi_predicted)
        
        mae_aqi = float(np.mean(np.abs(aqi_actual - aqi_predicted)))
        rmse_aqi = float(np.sqrt(np.mean((aqi_actual - aqi_predicted) ** 2)))
        
        ss_res_aqi = np.sum((aqi_actual - aqi_predicted) ** 2)
        ss_tot_aqi = np.sum((aqi_actual - np.mean(aqi_actual)) ** 2)
        r2_aqi = float(1 - (ss_res_aqi / ss_tot_aqi)) if ss_tot_aqi > 0 else 0
        
        mask_aqi = aqi_actual != 0
        mape_aqi = float(np.mean(np.abs((aqi_actual[mask_aqi] - aqi_predicted[mask_aqi]) / aqi_actual[mask_aqi])) * 100) if mask_aqi.any() else 0
        
        threshold_aqi = np.maximum(aqi_actual * 0.1, 5)
        accurate_aqi = np.abs(aqi_actual - aqi_predicted) <= threshold_aqi
        accuracy_aqi = float(np.mean(accurate_aqi) * 100)
        
        metrics['aqi'] = {
            'mae': round(mae_aqi, 2),
            'rmse': round(rmse_aqi, 2),
            'r2': round(r2_aqi, 4),
            'mape': round(mape_aqi, 2),
            'accuracy': round(accuracy_aqi, 2),
            'avgActual': round(float(np.mean(aqi_actual)), 2),
            'avgPredicted': round(float(np.mean(aqi_predicted)), 2),
            'samples': len(aqi_actual)
        }
        
        print(f"   AQI: MAE={mae_aqi:.2f}, RMSE={rmse_aqi:.2f}, R¬≤={r2_aqi:.4f}, Accuracy={accuracy_aqi:.1f}%")
        
        # 9. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≥—Ä–∞—Ñ—ñ–∫–∞
        print("\n9Ô∏è‚É£ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≥—Ä–∞—Ñ—ñ–∫–∞...")
        
        comparison_data = []
        
        for idx in range(len(test_df)):
            row = test_df.iloc[idx]
            
            actual_dict = {'aqi': float(aqi_actual[idx])}
            predicted_dict = {'aqi': float(aqi_predicted[idx])}
            
            for i, param in enumerate(parameters):
                actual_dict[param] = float(y_test[idx, i])
                predicted_dict[param] = float(predictions[idx, i])
            
            comparison_data.append({
                'timestamp': row['measured_at'].isoformat(),
                'actual': actual_dict,
                'predicted': predicted_dict
            })
        
        # DEBUG —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Å—Ç—Ä–∏–±–æ–∫
        print("\nüîç DEBUG - –ü–µ—Ä—à—ñ 5 –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ (PM2.5):")
        for idx in range(min(5, len(test_df))):
            row = test_df.iloc[idx]
            lag_val = row.get('pm25_lag_1', 0)
            print(f"   {row['measured_at'].strftime('%d-%m %H:%M')} ‚Üí "
                  f"Actual: {y_test[idx, 0]:.2f}, "
                  f"Predicted: {predictions[idx, 0]:.2f}, "
                  f"Lag_1: {lag_val:.2f}")
        
        # –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–±—ñ–ª—å—à–∏–π —Å—Ç—Ä–∏–±–æ–∫
        if len(test_df) > 1:
            diffs = np.abs(np.diff(y_test[:, 0]))
            max_jump_idx = np.argmax(diffs)
            
            print(f"\nüî• DEBUG - –ù–∞–π–±—ñ–ª—å—à–∏–π —Å—Ç—Ä–∏–±–æ–∫ PM2.5:")
            for offset in range(-2, 3):
                idx = max_jump_idx + offset
                if 0 <= idx < len(test_df):
                    row = test_df.iloc[idx]
                    lag_val = row.get('pm25_lag_1', 0)
                    marker = " ‚¨ÖÔ∏è –°–¢–†–ò–ë–û–ö" if idx == max_jump_idx else ""
                    print(f"   {row['measured_at'].strftime('%d-%m %H:%M')} ‚Üí "
                          f"Actual: {y_test[idx, 0]:.2f}, "
                          f"Predicted: {predictions[idx, 0]:.2f}, "
                          f"Lag_1: {lag_val:.2f}{marker}")
        
        # ========== –¢–ï–°–¢–ò –ù–ê DATA LEAKAGE ==========
        
        print(f"\n{'='*70}")
        print("üî¨ –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê DATA LEAKAGE")
        print(f"{'='*70}")
        
        # –¢–ï–°–¢ 1: Feature Importance
        print("\nüî¨ –¢–ï–°–¢ 1: Feature Importance (–¢–æ–ø-20)...")
        
        importances = []
        for estimator in model.model.estimators_:
            importances.append(estimator.feature_importances_)
        
        avg_importance = np.mean(importances, axis=0)
        
        importance_df = pd.DataFrame({
            'feature': feature_cols,
            'importance': avg_importance
        }).sort_values('importance', ascending=False)
        
        print("\n–¢–æ–ø-20 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö features:")
        for idx, row in importance_df.head(20).iterrows():
            print(f"   {row['feature']:35s} ‚Üí {row['importance']:.4f}")
        
        top_20_features = importance_df.head(20)['feature'].tolist()
        current_targets = [f for f in top_20_features if f in Config.TARGET_FEATURES]
        
        test1_passed = len(current_targets) == 0
        
        if test1_passed:
            print(f"\n   ‚úÖ –û–ö! –ù–µ–º–∞—î –ø–æ—Ç–æ—á–Ω–∏—Ö targets –≤ —Ç–æ–ø-20!")
        else:
            print(f"\n   ‚ùå –£–í–ê–ì–ê! –ü–æ—Ç–æ—á–Ω—ñ target features –≤ —Ç–æ–ø-20: {current_targets}")
        
        # –¢–ï–°–¢ 2: Persistence Baseline
        print("\nüî¨ –¢–ï–°–¢ 2: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ Persistence Baseline...")
        
        persistence_predictions = []
        for idx in range(len(test_df)):
            row = test_df.iloc[idx]
            pers_pred = []
            for param in parameters:
                lag_val = row.get(f'{param}_lag_1', 0)
                pers_pred.append(lag_val)
            persistence_predictions.append(pers_pred)
        
        persistence_predictions = np.array(persistence_predictions)
        
        model_mae = np.mean(np.abs(y_test[:, 0] - predictions[:, 0]))
        persistence_mae = np.mean(np.abs(y_test[:, 0] - persistence_predictions[:, 0]))
        
        improvement = ((persistence_mae - model_mae) / persistence_mae) * 100
        
        print(f"\n   PM2.5 MAE:")
        print(f"   Persistence (lag_1):  {persistence_mae:.2f}")
        print(f"   ML Model:             {model_mae:.2f}")
        print(f"   –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è:           {improvement:.1f}%")
        
        test2_passed = improvement > 10
        
        if test2_passed:
            print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ {improvement:.1f}% –∫—Ä–∞—â–∞ –∑–∞ naive baseline!")
        else:
            print(f"   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∫—Ä–∞—â–∞ ({improvement:.1f}%)")
        
        # –¢–ï–°–¢ 3: Random Shuffle Test
        print("\nüî¨ –¢–ï–°–¢ 3: Random Shuffle (–¥–µ—Ç–µ–∫—Ü—ñ—è temporal leakage)...")
        
        shuffle_idx = np.random.permutation(len(X_test_scaled))
        X_test_shuffled = X_test_scaled[shuffle_idx]
        y_test_shuffled = y_test[shuffle_idx]
        
        pred_shuffled = model.predict(X_test_shuffled)
        
        mae_normal = np.mean(np.abs(y_test[:, 0] - predictions[:, 0]))
        mae_shuffled = np.mean(np.abs(y_test_shuffled[:, 0] - pred_shuffled[:, 0]))
        
        mae_diff = abs(mae_shuffled - mae_normal)
        mae_diff_pct = (mae_diff / mae_normal) * 100
        
        print(f"\n   PM2.5 MAE:")
        print(f"   –ù–æ—Ä–º–∞–ª—å–Ω–∏–π –ø–æ—Ä—è–¥–æ–∫:   {mae_normal:.2f}")
        print(f"   –ü—ñ—Å–ª—è shuffle:        {mae_shuffled:.2f}")
        print(f"   –†—ñ–∑–Ω–∏—Ü—è:              {mae_diff:.2f} ({mae_diff_pct:.1f}%)")
        
        test3_passed = mae_diff_pct < 10
        
        if test3_passed:
            print(f"   ‚úÖ –†—ñ–∑–Ω–∏—Ü—è –º–∞–ª–∞ ({mae_diff_pct:.1f}%) - –Ω–µ–º–∞—î temporal leakage!")
        else:
            print(f"   ‚ö†Ô∏è –†—ñ–∑–Ω–∏—Ü—è –ø–æ–º—ñ—Ç–Ω–∞ ({mae_diff_pct:.1f}%)")
        
        # –¢–ï–°–¢ 4: Manual Check
        print("\nüî¨ –¢–ï–°–¢ 4: –†—É—á–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä—à–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑—É...")
        
        first_row = test_df.iloc[0]
        
        print(f"\n   –ß–∞—Å: {first_row['measured_at']}")
        print(f"   Actual PM2.5: {y_test[0, 0]:.2f}")
        print(f"   Predicted PM2.5: {predictions[0, 0]:.2f}")
        print(f"\n   –ü–µ—Ä—à—ñ 10 features:")
        
        first_features = X_test[0]
        for idx in range(min(10, len(feature_cols))):
            feat_name = feature_cols[idx]
            feat_value = first_features[idx]
            print(f"      {feat_name:30s} = {feat_value:.2f}")
        
        test4_passed = 'pm25' not in feature_cols
        
        if test4_passed:
            print(f"\n   ‚úÖ –û–ö! 'pm25' –Ω–µ –≤ features!")
        else:
            print(f"\n   ‚ùå LEAKAGE! 'pm25' –∑–Ω–∞–π–¥–µ–Ω–æ –≤ features!")
        
        # –¢–ï–°–¢ 5: Overfitting Check
        print("\nüî¨ –¢–ï–°–¢ 5: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ overfitting...")
        
        print(f"\n   Train R¬≤: {train_score:.4f}")
        print(f"   Test R¬≤:  {val_score:.4f}")
        print(f"   –†—ñ–∑–Ω–∏—Ü—è:  {abs(train_score - val_score):.4f}")
        
        test5_passed = train_score < 0.99 and abs(train_score - val_score) < 0.30
        
        if train_score > 0.99:
            print(f"   ‚ùå –ü–†–û–ë–õ–ï–ú–ê! Train R¬≤ = {train_score:.4f} (–∑–∞–Ω–∞–¥—Ç–æ –≤–∏—Å–æ–∫–∏–π!)")
            print(f"      –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:")
            print(f"      1. Target leakage")
            print(f"      2. Overfitting")
        elif abs(train_score - val_score) > 0.30:
            print(f"   ‚ö†Ô∏è –£–í–ê–ì–ê! –í–µ–ª–∏–∫–∞ —Ä—ñ–∑–Ω–∏—Ü—è train/test")
        else:
            print(f"   ‚úÖ –û–ö! –ù–æ—Ä–º–∞–ª—å–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏!")
        
        # –§–Ü–ù–ê–õ–¨–ù–ò–ô –í–ò–°–ù–û–í–û–ö
        print("\n" + "="*70)
        print("üéØ –§–Ü–ù–ê–õ–¨–ù–ê –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê")
        print("="*70)
        
        checks_passed = sum([test1_passed, test2_passed, test3_passed, test4_passed, test5_passed])
        total_checks = 5
        
        if test1_passed:
            print("‚úÖ –¢–µ—Å—Ç 1: –ù–µ–º–∞—î current targets –≤ —Ç–æ–ø features")
        else:
            print(f"‚ùå –¢–µ—Å—Ç 1: Current targets –∑–Ω–∞–π–¥–µ–Ω—ñ: {current_targets}")
        
        if test2_passed:
            print(f"‚úÖ –¢–µ—Å—Ç 2: –ú–æ–¥–µ–ª—å –Ω–∞ {improvement:.1f}% –∫—Ä–∞—â–∞ –∑–∞ baseline")
        else:
            print(f"‚ö†Ô∏è –¢–µ—Å—Ç 2: –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è {improvement:.1f}% (–º–∞–ª–æ)")
        
        if test3_passed:
            print(f"‚úÖ –¢–µ—Å—Ç 3: Shuffle test –ø—Ä–æ–π–¥–µ–Ω–æ ({mae_diff_pct:.1f}%)")
        else:
            print(f"‚ö†Ô∏è –¢–µ—Å—Ç 3: Shuffle —Ä—ñ–∑–Ω–∏—Ü—è {mae_diff_pct:.1f}%")
        
        if test4_passed:
            print("‚úÖ –¢–µ—Å—Ç 4: pm25 –Ω–µ –≤ features")
        else:
            print("‚ùå –¢–µ—Å—Ç 4: pm25 –∑–Ω–∞–π–¥–µ–Ω–æ –≤ features!")
        
        if test5_passed:
            print(f"‚úÖ –¢–µ—Å—Ç 5: –ù–µ–º–∞—î overfitting/leakage")
        else:
            print(f"‚ö†Ô∏è –¢–µ—Å—Ç 5: –ü—ñ–¥–æ–∑—Ä–∞ –Ω–∞ overfitting (Train R¬≤={train_score:.4f})")
        
        print(f"\n{'='*70}")
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢: {checks_passed}/{total_checks} —Ç–µ—Å—Ç—ñ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ")
        
        if checks_passed == total_checks:
            print("‚úÖ‚úÖ‚úÖ –í–°–ï –ß–£–î–û–í–û! –ú–æ–¥–µ–ª—å –ø–æ–≤–Ω—ñ—Å—Ç—é —á–∏—Å—Ç–∞!")
        elif checks_passed >= 4:
            print("‚úÖ‚ö†Ô∏è –ú–∞–π–∂–µ —ñ–¥–µ–∞–ª—å–Ω–æ, —î –Ω–µ–≤–µ–ª–∏–∫—ñ –∑–∞—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è")
        elif checks_passed >= 3:
            print("‚ö†Ô∏è –ü—Ä–∏–π–Ω—è—Ç–Ω–æ, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–≤–µ—Ä–Ω—É—Ç–∏ —É–≤–∞–≥—É")
        else:
            print("‚ùå –ö–†–ò–¢–ò–ß–ù–Ü –ü–†–û–ë–õ–ï–ú–ò! –ü–æ—Ç—Ä—ñ–±–Ω–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è!")
        
        print(f"{'='*70}\n")
        
        # 10. –ó–±–µ—Ä–µ–≥—Ç–∏ scaler
        print("üîü –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è scaler...")
        preprocessor.scaler = scaler
        import joblib
        import os
        os.makedirs(Config.MODEL_PATH, exist_ok=True)
        joblib.dump(scaler, preprocessor.scaler_path)
        print(f"‚úÖ Scaler –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {preprocessor.scaler_path}")
        
        print(f"\n{'='*70}")
        print("‚úÖ –¢–ï–°–¢–£–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*70}\n")
        
        # –î–æ–¥–∞—Ç–∏ diagnostic —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–æ response
        diagnostic_results = {
            'test1_feature_importance': bool(test1_passed),
            'test2_persistence': bool(test2_passed),
            'test2_improvement': round(float(improvement), 1),
            'test3_shuffle': bool(test3_passed),
            'test3_difference': round(float(mae_diff_pct), 1),
            'test4_manual': bool(test4_passed),
            'test5_overfitting': bool(test5_passed),
            'total_passed': int(checks_passed),
            'total_tests': int(total_checks),
            'top_features': [
                {
                    'feature': str(row['feature']),
                    'importance': round(float(row['importance']), 4)
                }
                for _, row in importance_df.head(20).iterrows()
            ]
        }

        # üîç –î–ï–¢–ê–õ–¨–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ê–ù–û–ú–ê–õ–Ü–á
        print("\n" + "="*70)
        print("üîç –î–ï–¢–ê–õ–¨–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ê–ù–û–ú–ê–õ–¨–ù–û–ì–û –°–¢–†–ò–ë–ö–ê")
        print("="*70)
        
        anomaly_details = None
        
        if len(test_df) > 1:
            diffs = np.abs(np.diff(y_test[:, 0]))
            max_jump_idx = np.argmax(diffs) + 1  # ‚úÖ +1 —â–æ–± –ø–æ–∫–∞–∑–∞—Ç–∏ –ù–ê–°–¢–£–ü–ù–£ —Ç–æ—á–∫—É!
            
            print(f"\nüìç –ê–Ω–æ–º–∞–ª—ñ—è –Ω–∞ —ñ–Ω–¥–µ–∫—Å—ñ {max_jump_idx}")
            
            # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—é —Ç–æ—á–∫—É –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            if max_jump_idx > 0:
                prev_row = test_df.iloc[max_jump_idx - 1]
                print(f"   –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –º–æ–º–µ–Ω—Ç: {prev_row['measured_at'].strftime('%d-%m %H:%M')}")
                print(f"      PM2.5: {y_test[max_jump_idx - 1, 0]:.2f}")
            
            # –ü–æ–∫–∞–∑–∞—Ç–∏ –í–°–Ü –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ –º–æ–º–µ–Ω—Ç —Å—Ç—Ä–∏–±–∫–∞
            anomaly_row = test_df.iloc[max_jump_idx]
            
            print(f"\nüî• –ú–æ–º–µ–Ω—Ç —Å—Ç—Ä–∏–±–∫–∞: {anomaly_row['measured_at'].strftime('%d-%m %H:%M')}")
            print("\n–í–°–Ü –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ actual vs predicted:")
            
            anomaly_params = []
            for i, param in enumerate(parameters):
                actual = y_test[max_jump_idx, i]
                predicted = predictions[max_jump_idx, i]
                lag_1 = anomaly_row.get(f'{param}_lag_1', 0)
                
                # ‚úÖ –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ –ü–û–ü–ï–†–ï–î–ù–Ü–ú –∑–Ω–∞—á–µ–Ω–Ω—è–º
                if max_jump_idx > 0:
                    prev_actual = y_test[max_jump_idx - 1, i]
                    jump_size = actual / prev_actual if prev_actual > 0.01 else 0
                else:
                    jump_size = actual / lag_1 if lag_1 > 0.01 else 0
                
                anomaly_params.append({
                    'param': param,
                    'actual': float(actual),
                    'predicted': float(predicted),
                    'lag_1': float(lag_1),
                    'jump_ratio': float(jump_size)
                })
                
                print(f"   {param:6s}: Actual={actual:7.2f}, "
                      f"Predicted={predicted:7.2f}, "
                      f"Lag_1={lag_1:7.2f}, "
                      f"–°—Ç—Ä–∏–±–æ–∫: {jump_size:.1f}x")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –í–°–Ü –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Å—Ç—Ä–∏–±–Ω—É–ª–∏ –æ–¥–Ω–æ—á–∞—Å–Ω–æ
            print("\nüî¨ –ê–Ω–∞–ª—ñ–∑:")
            
            jumps = []
            for entry in anomaly_params:
                jumps.append((entry['param'], entry['jump_ratio']))
            
            # –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º —Å—Ç—Ä–∏–±–∫–∞
            jumps.sort(key=lambda x: x[1], reverse=True)
            
            print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º —Å—Ç—Ä–∏–±–∫–∞:")
            for param, ratio in jumps:
                if ratio > 2:
                    print(f"   ‚ö†Ô∏è {param:6s}: —Å—Ç—Ä–∏–±–æ–∫ —É {ratio:.1f} —Ä–∞–∑—ñ–≤! ‚¨ÜÔ∏è")
                elif ratio < 0.5:
                    print(f"   ‚¨áÔ∏è {param:6s}: –≤–ø–∞–≤ —É {1/ratio:.1f} —Ä–∞–∑—ñ–≤!")
                else:
                    print(f"   ‚úÖ {param:6s}: —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π ({ratio:.2f}x)")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —Ü–µ –±—É–≤ default value (–±–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —Å—Ç—Ä–∏–±–Ω—É–ª–∏)
            print("\nü§î –ì–Ü–ü–û–¢–ï–ó–ê: –ß–∏ —Ü–µ default values –≤—ñ–¥ API?")
            
            # –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ —Å–∫—ñ–ª—å–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –†–Ü–ó–ö–û –∑–º—ñ–Ω–∏–ª–∏—Å—å
            significant_jumps = sum(1 for _, r in jumps if r > 1.5 or r < 0.7)
            
            is_api_glitch = False
            if significant_jumps >= 4:
                print(f"   ‚ùó {significant_jumps} –∑ {len(jumps)} –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –†–Ü–ó–ö–û —Å—Ç—Ä–∏–±–Ω—É–ª–∏!")
                print("   ‚ö†Ô∏è –ü–Ü–î–û–ó–†–ê: –¶–µ —Å—Ö–æ–∂–µ –Ω–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π –∑–±—ñ–π API!")
                print("   üí° –ü–æ—è—Å–Ω–µ–Ω–Ω—è:")
                print("      - –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –ù–ï –∑–º—ñ–Ω—é—é—Ç—å—Å—è –æ–¥–Ω–æ—á–∞—Å–Ω–æ")
                print("      - PM2.5, PM10, NO2, SO2, CO, O3 –º–∞—é—Ç—å —Ä—ñ–∑–Ω—É –¥–∏–Ω–∞–º—ñ–∫—É")
                print("      - –û–¥–Ω–æ—á–∞—Å–Ω–∞ –∑–º—ñ–Ω–∞ = API –ø–æ–≤–µ—Ä–Ω—É–≤ –¥–µ—Ñ–æ–ª—Ç–Ω—ñ/–ø–æ–º–∏–ª–∫–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è")
                print("")
                print("   üîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–Ø: –í–∏–¥–∞–ª–∏—Ç–∏ —Ü—ñ –∑–∞–ø–∏—Å–∏ –∑ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö!")
                is_api_glitch = True
            else:
                print(f"   ‚úÖ –¢—ñ–ª—å–∫–∏ {significant_jumps} –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —Å—Ç—Ä–∏–±–Ω—É–ª–∏")
                print("   ‚ÑπÔ∏è –°—Ö–æ–∂–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É –ø–æ–¥—ñ—é (–Ω–∞–ø—Ä. –≤–∏–∫–∏–¥ –∑–∞–±—Ä—É–¥–Ω—é–≤–∞—á—ñ–≤)")
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —á–∏ –∑–Ω–∞—á–µ–Ω–Ω—è "–æ–∫—Ä—É–≥–ª–µ–Ω—ñ"?
            print(f"\nüîé –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:")
            for i, param in enumerate(parameters):
                actual = y_test[max_jump_idx, i]
                # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –∑–Ω–∞—á–µ–Ω–Ω—è "–ø—ñ–¥–æ–∑—Ä—ñ–ª–æ –æ–∫—Ä—É–≥–ª–µ–Ω–µ" (0, 5, 10, 100 —Ç–æ—â–æ)
                if actual in [0, 1, 5, 10, 50, 100, 200, 500]:
                    print(f"   ‚ö†Ô∏è {param}: {actual} (–ø—ñ–¥–æ–∑—Ä—ñ–ª–æ –æ–∫—Ä—É–≥–ª–µ–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è!)")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ timestamp
            anomaly_time = anomaly_row['measured_at']
            train_end_time = train_df['measured_at'].max()
            
            print(f"\nüìÖ –ß–∞—Å–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞:")
            print(f"   Train –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è: {train_end_time.strftime('%d-%m %H:%M')}")
            print(f"   –ê–Ω–æ–º–∞–ª—ñ—è —Å—Ç–∞–ª–∞—Å—å: {anomaly_time.strftime('%d-%m %H:%M')}")
            
            time_diff = (anomaly_time - train_end_time).total_seconds() / 3600
            print(f"   –†—ñ–∑–Ω–∏—Ü—è: {time_diff:.1f} –≥–æ–¥–∏–Ω –ø—ñ—Å–ª—è train")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –º–æ–¥–µ–ª—å –±–∞—á–∏–ª–∞ —Ç–∞–∫—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ä–∞–Ω—ñ—à–µ
            if max_jump_idx > 0:
                prev_pm25 = y_test[max_jump_idx - 1, 0]
                curr_pm25 = y_test[max_jump_idx, 0]
                predicted_pm25 = predictions[max_jump_idx, 0]
                
                print(f"\nü§ñ –ß–æ–º—É –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥–±–∞—á–∏–ª–∞ {predicted_pm25:.1f}?")
                print(f"   –ü–æ–ø–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è: {prev_pm25:.1f}")
                print(f"   –§–∞–∫—Ç–∏—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è:  {curr_pm25:.1f}")
                print(f"   –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥–±–∞—á–∏–ª–∞: {predicted_pm25:.1f}")
                
                if predicted_pm25 > prev_pm25 * 2:
                    print(f"\n   üí° –ú–æ–¥–µ–ª—å –ø–æ–±–∞—á–∏–ª–∞ –©–û–°–¨ –≤ features —â–æ –≤–∫–∞–∑—É–≤–∞–ª–æ –Ω–∞ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è:")
                    print(f"      - –ú–æ–∂–ª–∏–≤–æ —ñ–Ω—à—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (CO, NO2) –≤–∂–µ –ø–æ—á–∞–ª–∏ —Ä–æ—Å—Ç–∏")
                    print(f"      - –ú–æ–∂–ª–∏–≤–æ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ (–≤—ñ—Ç–µ—Ä –≤–ø–∞–≤, –≤–æ–ª–æ–≥—ñ—Å—Ç—å –∑—Ä–æ—Å–ª–∞)")
                    print(f"      - –ú–æ–∂–ª–∏–≤–æ EWM/rolling features –ø–æ–∫–∞–∑–∞–ª–∏ —Ç—Ä–µ–Ω–¥")
                    
                    # –ü–æ–∫–∞–∑–∞—Ç–∏ —è–∫—ñ features –Ω–∞–π–±—ñ–ª—å—à–µ –≤–ø–ª–∏–Ω—É–ª–∏
                    print(f"\n   üìä –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ features –¥–ª—è —Ü—å–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑—É:")
                    print(f"      (features –∑ importance > 0.05)")
                    for idx, row_imp in importance_df.iterrows():
                        if row_imp['importance'] > 0.05:
                            feat_name = row_imp['feature']
                            feat_value = anomaly_row.get(feat_name, 0)
                            print(f"         {feat_name:30s} = {feat_value:.2f} (importance: {row_imp['importance']:.3f})")
            
            if time_diff > gap_size / 24:
                print(f"   ‚úÖ –ê–Ω–æ–º–∞–ª—ñ—è –≤ test –¥–∞–Ω–∏—Ö (–ø—ñ—Å–ª—è gap)")
            else:
                print(f"   ‚ö†Ô∏è –ê–Ω–æ–º–∞–ª—ñ—è –±–ª–∏–∑—å–∫–æ –¥–æ train –¥–∞–Ω–∏—Ö")
            
            # –ó–±–µ—Ä–µ–≥—Ç–∏ –¥–µ—Ç–∞–ª—ñ –¥–ª—è response
            anomaly_details = {
                'timestamp': anomaly_time.isoformat(),
                'index': int(max_jump_idx),
                'parameters': anomaly_params,
                'jumped_count': int(significant_jumps),
                'total_params': len(jumps),
                'is_api_glitch': is_api_glitch,
                'hours_after_train': float(time_diff)
            }
        
        print(f"{'='*70}\n")
        
        # –î–æ–¥–∞—Ç–∏ anomaly_details –¥–æ diagnostic_results
        diagnostic_results = {
            'test1_feature_importance': bool(test1_passed),
            'test2_persistence': bool(test2_passed),
            'test2_improvement': round(float(improvement), 1),
            'test3_shuffle': bool(test3_passed),
            'test3_difference': round(float(mae_diff_pct), 1),
            'test4_manual': bool(test4_passed),
            'test5_overfitting': bool(test5_passed),
            'total_passed': int(checks_passed),
            'total_tests': int(total_checks),
            'anomaly_analysis': anomaly_details,  # ‚úÖ –î–æ–¥–∞–Ω–æ!
            'top_features': [
                {
                    'feature': str(row['feature']),
                    'importance': round(float(row['importance']), 4)
                }
                for _, row in importance_df.head(20).iterrows()
            ]
        }
        
        return jsonify({
            'success': True,
            'district_id': district_id,
            'metrics': metrics,
            'comparison_data': comparison_data,
            'diagnostic': diagnostic_results,
            'data_info': {
                'total_samples': len(df),
                'train_samples': train_size,
                'gap_samples': gap_size,
                'test_samples': len(test_df),
                'features_count': len(feature_cols),
                'date_range': {
                    'start': df['measured_at'].min().isoformat(),
                    'end': df['measured_at'].max().isoformat()
                }
            }
        })
    
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {str(e)}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/test-data-info/<int:district_id>', methods=['GET'])
def get_test_data_info(district_id):
    """–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –¥–æ—Å—Ç—É–ø–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
    try:
        query = """
            SELECT COUNT(*) as total_records,
                   MIN(measured_at) as first_date,
                   MAX(measured_at) as last_date,
                   COUNT(DISTINCT DATE(measured_at)) as days_with_data
            FROM air_quality_history
            WHERE district_id = %s AND is_forecast = false
        """
        
        conn = db.get_connection()
        cursor = conn.cursor()
        cursor.execute(query, (district_id,))
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        return jsonify({
            'success': True,
            'district_id': district_id,
            'total_records': result[0],
            'first_date': result[1].isoformat() if result[1] else None,
            'last_date': result[2].isoformat() if result[2] else None,
            'days_with_data': result[3]
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/test-scenario', methods=['POST'])
def test_scenario():
    """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–º—É —Å—Ü–µ–Ω–∞—Ä—ñ—ó"""
    try:
        data = request.json
        district_id = data.get('district_id')
        scenario = data.get('scenario', 'fire')
        custom_values = data.get('custom_values')
        
        print(f"\n{'='*70}")
        print(f"üî• –°–¶–ï–ù–ê–†–ù–ò–ô –¢–ï–°–¢ - –†–∞–π–æ–Ω {district_id}, –°—Ü–µ–Ω–∞—Ä—ñ–π: {scenario}")
        print(f"{'='*70}")
        
        from data.preprocessor import DataPreprocessor
        from models.air_quality_model import AirQualityModel
        
        print("\n1Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
        model = AirQualityModel(district_id, model_type='xgboost')
        
        if not model.load_model():
            return jsonify({
                'success': False,
                'error': '–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∞. –°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç—ñ—Ç—å —Ç–µ—Å—Ç –º–æ–¥–µ–ª—ñ.'
            }), 400
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
        
        print("\n2Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É...")
        query = """
            SELECT pm25, pm10, no2, so2, co, o3,
                   temperature, humidity, pressure, 
                   wind_speed, wind_direction, measured_at
            FROM air_quality_history
            WHERE district_id = %s AND is_forecast = false
            ORDER BY measured_at DESC
            LIMIT 50
        """
        
        conn = db.get_connection()
        df_context = pd.read_sql_query(query, conn, params=(district_id,))
        conn.close()
        
        if len(df_context) < 10:
            return jsonify({
                'success': False,
                'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É'
            }), 400
        
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df_context)} –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É")
        
        print(f"\n3Ô∏è‚É£ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä—ñ—é: {scenario}")
        
        last_record = df_context.iloc[0].copy()
        
        if custom_values:
            extreme_values = custom_values
        else:
            scenarios_values = {
                'fire': {
                    'pm25': 250, 'pm10': 300, 'no2': 80, 'so2': 50,
                    'co': 2000, 'o3': 120, 'temperature': 28,
                    'humidity': 45, 'wind_speed': 2
                },
                'industrial_accident': {
                    'pm25': 80, 'pm10': 120, 'no2': 200, 'so2': 150,
                    'co': 3500, 'o3': 40, 'temperature': 22,
                    'humidity': 55, 'wind_speed': 3
                },
                'heavy_fog': {
                    'pm25': 65, 'pm10': 150, 'no2': 60, 'so2': 40,
                    'co': 800, 'o3': 30, 'temperature': 8,
                    'humidity': 95, 'wind_speed': 0.5
                },
                'strong_wind': {
                    'pm25': 12, 'pm10': 25, 'no2': 20, 'so2': 15,
                    'co': 400, 'o3': 60, 'temperature': 18,
                    'humidity': 60, 'wind_speed': 15
                },
                'normal': {
                    'pm25': 25, 'pm10': 40, 'no2': 35, 'so2': 25,
                    'co': 600, 'o3': 70, 'temperature': 15,
                    'humidity': 65, 'wind_speed': 5
                }
            }
            extreme_values = scenarios_values.get(scenario, scenarios_values['fire'])
        
        extreme_record = last_record.copy()
        for key, value in extreme_values.items():
            if key in extreme_record.index:
                extreme_record[key] = value
        
        extreme_record['measured_at'] = pd.Timestamp.now()
        
        df_with_extreme = pd.concat([
            pd.DataFrame([extreme_record]),
            df_context
        ], ignore_index=True)
        
        print("‚úÖ –ï–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:")
        for key, value in extreme_values.items():
            print(f"   {key}: {value}")
        
        print("\n4Ô∏è‚É£ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ features...")
        preprocessor = DataPreprocessor(district_id)
        
        import os
        import joblib
        scaler_path = f"{Config.MODEL_PATH}/scaler_district_{district_id}.pkl"
        
        if not os.path.exists(scaler_path):
            return jsonify({
                'success': False,
                'error': 'Scaler –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç—ñ—Ç—å —Ç–µ—Å—Ç –º–æ–¥–µ–ª—ñ.'
            }), 400
        
        preprocessor.scaler = joblib.load(scaler_path)
        
        # –û—Ç—Ä–∏–º–∞—Ç–∏ –Ω–∞–∑–≤–∏ –∫–æ–ª–æ–Ω–æ–∫ features
        feature_cols = preprocessor.get_feature_columns()
        
        print(f"‚úÖ Scaler –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        
        # 5. –Ü–¢–ï–†–ê–¢–ò–í–ù–ï –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω—ñ 12 –≥–æ–¥–∏–Ω
        print("\n5Ô∏è‚É£ –Ü–¢–ï–†–ê–¢–ò–í–ù–ï –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö 12 –≥–æ–¥–∏–Ω...")
        
        forecasts = []
        current_time = pd.Timestamp.now()
        parameters = ['pm25', 'pm10', 'no2', 'so2', 'co', 'o3']
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ df_with_extreme –¥–ª—è —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑—É
        working_df = df_with_extreme.copy()
        
        for hour in range(1, 13):
            print(f"   –ì–æ–¥–∏–Ω–∞ {hour}...")
            
            # 1. –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ features –∑ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å—Ç–∞–Ω—É
            df_features = preprocessor.prepare_features(working_df)
            
            if len(df_features) == 0:
                print(f"   ‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ features –¥–ª—è –≥–æ–¥–∏–Ω–∏ {hour}")
                break
            
            # 2. –í–∑—è—Ç–∏ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ (–Ω–∞–π—Å–≤—ñ–∂—ñ—à—ñ –¥–∞–Ω—ñ)
            X_current = df_features[feature_cols].iloc[0:1].values
            X_current_scaled = preprocessor.scaler.transform(X_current)
            
            # 3. –ó—Ä–æ–±–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑
            prediction = model.predict(X_current_scaled)[0]
            
            # 4. –°—Ç–≤–æ—Ä–∏—Ç–∏ –∑–∞–ø–∏—Å –ø—Ä–æ–≥–Ω–æ–∑—É
            forecast_time = current_time + timedelta(hours=hour)
            
            forecast_dict = {
                'timestamp': forecast_time.isoformat(),
                'hour': hour
            }
            
            for i, param in enumerate(parameters):
                forecast_dict[param] = round(float(prediction[i]), 2)
            
            # –†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ AQI
            aqi = calculate_aqi_from_pm25(forecast_dict['pm25'])
            forecast_dict['aqi'] = int(aqi)
            forecast_dict['aqi_status'] = get_aqi_status(aqi)
            
            forecasts.append(forecast_dict)
            
            # 5. –í–ê–ñ–õ–ò–í–û: –û–Ω–æ–≤–∏—Ç–∏ —Ä–æ–±–æ—á–∏–π DataFrame
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π –∑–∞–ø–∏—Å –∑ –ø—Ä–æ–≥–Ω–æ–∑–æ–º
            new_row = working_df.iloc[0].copy()
            new_row['measured_at'] = forecast_time
            
            # –û–Ω–æ–≤–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            for i, param in enumerate(parameters):
                new_row[param] = prediction[i]
            
            # –î–æ–¥–∞—Ç–∏ –Ω–æ–≤–∏–π –∑–∞–ø–∏—Å –Ω–∞ –ø–æ—á–∞—Ç–æ–∫ DataFrame
            working_df = pd.concat([
                pd.DataFrame([new_row]),
                working_df
            ], ignore_index=True)
            
            # –û–±–º–µ–∂–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä (–∑–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ 50 –∑–∞–ø–∏—Å—ñ–≤)
            working_df = working_df.head(50)
            
            # –ü–æ–∫–∞–∑–∞—Ç–∏ —â–æ —Å–ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–ª–æ—Å—å
            if hour <= 3 or hour == 12:  # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–µ—Ä—à—ñ 3 —ñ –æ—Å—Ç–∞–Ω–Ω—é
                pm25_val = forecast_dict['pm25']
                co_val = forecast_dict['co']
                print(f"      ‚Üí PM2.5: {pm25_val:.1f}, CO: {co_val:.1f}, AQI: {forecast_dict['aqi']}")
        
        print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ {len(forecasts)} —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤")
        
        print("\n6Ô∏è‚É£ –ê–Ω–∞–ª—ñ–∑ —Ç—Ä–µ–Ω–¥—É –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö...")
        
        SAFE_THRESHOLDS = {
            'pm25': 12.0, 'pm10': 50.0, 'no2': 40.0,
            'so2': 20.0, 'co': 4000.0, 'o3': 100.0
        }
        
        MODERATE_THRESHOLDS = {
            'pm25': 35.4, 'pm10': 154.0, 'no2': 100.0,
            'so2': 75.0, 'co': 9400.0, 'o3': 140.0
        }
        
        CRITICAL_THRESHOLDS = {
            'pm25': 150.4, 'pm10': 254.0, 'no2': 200.0,
            'so2': 185.0, 'co': 15400.0, 'o3': 200.0
        }
        
        parameter_analysis = {}
        
        for param in parameters:
            initial_value = extreme_values[param]
            final_value = forecasts[-1][param]
            
            safe_time = None
            moderate_time = None
            
            for i, f in enumerate(forecasts):
                if f[param] <= SAFE_THRESHOLDS[param] and safe_time is None:
                    safe_time = i + 1
                if f[param] <= MODERATE_THRESHOLDS[param] and moderate_time is None:
                    moderate_time = i + 1
            
            initial_status = 'critical' if initial_value > CRITICAL_THRESHOLDS[param] else \
                           'high' if initial_value > MODERATE_THRESHOLDS[param] else \
                           'moderate' if initial_value > SAFE_THRESHOLDS[param] else 'safe'
            
            final_status = 'critical' if final_value > CRITICAL_THRESHOLDS[param] else \
                         'high' if final_value > MODERATE_THRESHOLDS[param] else \
                         'moderate' if final_value > SAFE_THRESHOLDS[param] else 'safe'
            
            percent_change = ((final_value - initial_value) / initial_value * 100) if initial_value > 0 else 0
            
            parameter_analysis[param] = {
                'initial_value': round(float(initial_value), 2),
                'final_value': round(float(final_value), 2),
                'initial_status': initial_status,
                'final_status': final_status,
                'percent_change': round(percent_change, 1),
                'safe_threshold': SAFE_THRESHOLDS[param],
                'moderate_threshold': MODERATE_THRESHOLDS[param],
                'critical_threshold': CRITICAL_THRESHOLDS[param],
                'time_to_safe': safe_time,
                'time_to_moderate': moderate_time,
                'will_be_safe': safe_time is not None,
                'will_be_moderate': moderate_time is not None
            }
            
            print(f"   {param.upper()}: {initial_value:.1f} ‚Üí {final_value:.1f} ({percent_change:+.1f}%)")
            if safe_time:
                print(f"      ‚úÖ –ë–µ–∑–ø–µ—á–Ω–∏–π —Ä—ñ–≤–µ–Ω—å —á–µ—Ä–µ–∑ {safe_time} –≥–æ–¥")
            elif moderate_time:
                print(f"      ‚ö†Ô∏è –ü–æ–º—ñ—Ä–Ω–∏–π —Ä—ñ–≤–µ–Ω—å —á–µ—Ä–µ–∑ {moderate_time} –≥–æ–¥")
            else:
                print(f"      ‚ùå –ù–µ –¥–æ—Å—è–≥–Ω–µ –±–µ–∑–ø–µ—á–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è")
        
        initial_aqi = int(calculate_aqi_from_pm25(extreme_values['pm25']))
        final_aqi = forecasts[-1]['aqi']
        max_aqi = max(f['aqi'] for f in forecasts)
        min_aqi = min(f['aqi'] for f in forecasts)
        
        slowest_recovery = None
        slowest_recovery_time = 0
        
        for param, info in parameter_analysis.items():
            if info['will_be_safe'] and info['time_to_safe'] > slowest_recovery_time:
                slowest_recovery = param
                slowest_recovery_time = info['time_to_safe']
        
        all_parameters_safe = all(info['will_be_safe'] for info in parameter_analysis.values())
        
        critical_pollutants = [
            param for param, info in parameter_analysis.items() 
            if info['initial_status'] == 'critical'
        ]
        
        analysis = {
            'initial_aqi': initial_aqi,
            'final_aqi': final_aqi,
            'max_aqi': max_aqi,
            'min_aqi': min_aqi,
            'trend': 'improving' if final_aqi < initial_aqi else 'worsening' if final_aqi > initial_aqi else 'stable',
            'all_parameters_safe': all_parameters_safe,
            'slowest_recovery': slowest_recovery,
            'slowest_recovery_time': slowest_recovery_time,
            'critical_pollutants': critical_pollutants,
            'parameter_details': parameter_analysis
        }
        
        print(f"\n   –ó–∞–≥–∞–ª—å–Ω–∏–π —Ç—Ä–µ–Ω–¥: {analysis['trend']}")
        print(f"   AQI: {initial_aqi} ‚Üí {final_aqi}")
        if all_parameters_safe:
            print(f"   ‚úÖ –í—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–æ—Å—è–≥–Ω—É—Ç—å –±–µ–∑–ø–µ—á–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è")
            print(f"   ‚è±Ô∏è –ù–∞–π–ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è: {slowest_recovery} ({slowest_recovery_time} –≥–æ–¥)")
        else:
            print(f"   ‚ö†Ô∏è –ù–µ –≤—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–æ—Å—è–≥–Ω—É—Ç—å –±–µ–∑–ø–µ—á–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è –∑–∞ 12 –≥–æ–¥–∏–Ω")
        
        print(f"\n{'='*70}")
        print("‚úÖ –°–¶–ï–ù–ê–†–ù–ò–ô –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*70}\n")
        
        return jsonify({
            'success': True,
            'district_id': district_id,
            'scenario': scenario,
            'initial_values': extreme_values,
            'forecasts': forecasts,
            'analysis': analysis
        })
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ü–µ–Ω–∞—Ä–Ω–æ–≥–æ —Ç–µ—Å—Ç—É: {str(e)}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

# ==================== MAIN ====================

if __name__ == '__main__':
    print("=" * 60)
    print("üöÄ ML SERVICE –ó–ê–ü–£–©–ï–ù–û")
    print("=" * 60)
    print(f"üåê URL: http://localhost:{Config.FLASK_PORT}")
    print(f"üìä Endpoints:")
    print(f"   GET  /health")
    print(f"   GET  /api/predict/<district_id>?hours=24")
    print(f"   GET  /api/predict/all?hours=24")
    print(f"   GET  /api/model/<district_id>/info")
    print(f"   POST /test-model")
    print(f"   GET  /test-data-info/<district_id>")
    print(f"   POST /test-scenario")
    print("=" * 60)
    
    app.run(
        host='0.0.0.0',
        port=Config.FLASK_PORT,
        debug=Config.FLASK_DEBUG
    )