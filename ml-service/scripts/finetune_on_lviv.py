# ml-service/scripts/finetune_on_lviv.py
"""
Fine-tuning –º–æ–¥–µ–ª—ñ –Ω–∞ –¥–∞–Ω–∏—Ö –õ—å–≤–æ–≤–∞
"""
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.db_helper import DatabaseHelper
from data.preprocessor import DataPreprocessor
from models.lstm_model import LSTMForecastModel
from config import Config
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import load_model

class LvivFineTuner:
    """Fine-tuning –Ω–∞ –¥–∞–Ω–∏—Ö –õ—å–≤–æ–≤–∞"""
    
    def __init__(self, district_id, pretrained_model_path):
        self.district_id = district_id
        self.pretrained_model_path = pretrained_model_path
        self.db = DatabaseHelper()
    
    def load_pretrained_model(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ pre-trained –º–æ–¥–µ–ª—å"""
        print(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è pre-trained –º–æ–¥–µ–ª—ñ: {self.pretrained_model_path}")
        
        if not os.path.exists(self.pretrained_model_path):
            print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç–∏ pre-training.")
            return None
        
        model = load_model(self.pretrained_model_path)
        print("‚úÖ Pre-trained –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        
        return model
    
    def prepare_lviv_data(self, days=30):
        """–ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –õ—å–≤–æ–≤–∞"""
        print(f"\nüìä –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –õ—å–≤–æ–≤–∞ (—Ä–∞–π–æ–Ω {self.district_id})...")
        
        # –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞–Ω—ñ –∑ –ë–î
        df = self.db.get_historical_data(self.district_id, days=days)
        
        if len(df) < Config.SEQUENCE_LENGTH + 1:
            print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö! –ü–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º {Config.SEQUENCE_LENGTH + 1} –∑–∞–ø–∏—Å—ñ–≤")
            return None, None
        
        print(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
        
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥
        preprocessor = DataPreprocessor(self.district_id)
        prepared_data = preprocessor.prepare_data(df)
        normalized_data = preprocessor.normalize_data(prepared_data.values, fit=True)
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        X, y = preprocessor.create_sequences(normalized_data, Config.SEQUENCE_LENGTH)
        
        print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ {len(X)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π")
        
        return X, y
    
    def finetune(self, pretrained_model, X_train, y_train, X_val, y_val, epochs=10):
        """Fine-tuning –º–æ–¥–µ–ª—ñ"""
        print("\nüîß FINE-TUNING –ù–ê –î–ê–ù–ò–• –õ–¨–í–û–í–ê")
        print("=" * 70)
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±–≥–æ—Ä—Ç–∫—É –¥–ª—è –º–æ–¥–µ–ª—ñ
        model_wrapper = LSTMForecastModel(self.district_id)
        model_wrapper.model = pretrained_model
        
        # –ó–∞–º–æ—Ä–æ–∑–∏—Ç–∏ —á–∞—Å—Ç–∏–Ω—É —à–∞—Ä—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        # for layer in model_wrapper.model.layers[:-4]:  # –ó–∞–º–æ—Ä–æ–∑–∏—Ç–∏ –≤—Å—ñ –∫—Ä—ñ–º –æ—Å—Ç–∞–Ω–Ω—ñ—Ö 4
        #     layer.trainable = False
        
        # –ü–µ—Ä–µ–∫–æ–º–ø—ñ–ª—é–≤–∞—Ç–∏ –∑ –º–µ–Ω—à–∏–º learning rate
        model_wrapper.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # –ú–µ–Ω—à–∏–π LR
            loss={
                'pm25': 'mse',
                'pm10': 'mse',
                'no2': 'mse',
                'so2': 'mse',
                'co': 'mse',
                'o3': 'mse'
            }
        )
        
        print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ fine-tuning:")
        print(f"   üî¢ –ï–ø–æ—Ö–∏: {epochs}")
        print(f"   üìä Train: {len(X_train)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π")
        print(f"   üìä Val: {len(X_val)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π")
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        history = model_wrapper.train(
            X_train, y_train,
            X_val, y_val,
            epochs=epochs,
            batch_size=16
        )
        
        print(f"\n‚úÖ Fine-tuning –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   üíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {model_wrapper.model_path}")
        
        return model_wrapper
    
    def run(self, days=30, epochs=10):
        """–ó–∞–ø—É—Å—Ç–∏—Ç–∏ fine-tuning"""
        print("\n" + "=" * 70)
        print(f"üéØ FINE-TUNING –î–õ–Ø –†–ê–ô–û–ù–£ {self.district_id}")
        print("=" * 70)
        
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ pre-trained –º–æ–¥–µ–ª—å
        pretrained_model = self.load_pretrained_model()
        if pretrained_model is None:
            return
        
        # 2. –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –õ—å–≤–æ–≤–∞
        X, y = self.prepare_lviv_data(days=days)
        if X is None:
            return
        
        # 3. –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –Ω–∞ train/val
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        
        # 4. Fine-tuning
        model = self.finetune(
            pretrained_model,
            X_train, y_train,
            X_val, y_val,
            epochs=epochs
        )
        
        print("\n" + "=" * 70)
        print("‚úÖ FINE-TUNING –ó–ê–í–ï–†–®–ï–ù–û!")
        print("=" * 70)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Fine-tuning –Ω–∞ –¥–∞–Ω–∏—Ö –õ—å–≤–æ–≤–∞')
    parser.add_argument('--district', type=int, required=True, help='ID —Ä–∞–π–æ–Ω—É (1-6)')
    parser.add_argument('--pretrained', type=str, 
                       default='models/lstm_pretrained_saveecobot.keras',
                       help='–®–ª—è—Ö –¥–æ pre-trained –º–æ–¥–µ–ª—ñ')
    parser.add_argument('--days', type=int, default=30, help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–Ω—ñ–≤ –¥–∞–Ω–∏—Ö')
    parser.add_argument('--epochs', type=int, default=10, help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö')
    
    args = parser.parse_args()
    
    finetuner = LvivFineTuner(args.district, args.pretrained)
    finetuner.run(days=args.days, epochs=args.epochs)