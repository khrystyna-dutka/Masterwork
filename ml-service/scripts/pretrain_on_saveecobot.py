# ml-service/scripts/pretrain_on_saveecobot.py
"""
–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –¥–∞–Ω–∏—Ö SaveEcoBot
"""
import pandas as pd
import numpy as np
import sys
import os
from datetime import datetime

# –î–æ–¥–∞—Ç–∏ —à–ª—è—Ö –¥–æ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.lstm_model import LSTMForecastModel
from data.preprocessor import DataPreprocessor
from config import Config
from sklearn.model_selection import train_test_split
import tensorflow as tf

class SaveEcoBotPretrainer:
    """–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –¥–∞–Ω–∏—Ö SaveEcoBot"""
    
    def __init__(self, csv_path):
        self.csv_path = csv_path
        self.sequence_length = Config.SEQUENCE_LENGTH
    
    def load_and_prepare_data(self, sample_size=None, min_records_per_city=100):
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–∞ –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ CSV
        
        Args:
            sample_size: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –º—ñ—Å—Ç –¥–ª—è –≤–∏–±—ñ—Ä–∫–∏ (None = –≤—Å—ñ)
            min_records_per_city: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è –º—ñ—Å—Ç–∞
        """
        print("=" * 70)
        print("üìÇ –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–ù–ò–• SAVEECOBOT")
        print("=" * 70)
        
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV
        print(f"\nüì• –ß–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É: {self.csv_path}")
        df = pd.read_csv(self.csv_path)
        
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"üìä –ö–æ–ª–æ–Ω–∫–∏: {', '.join(df.columns)}")
        
        # 2. –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –¥–∞—Ç—É
        df['logged_at'] = pd.to_datetime(df['logged_at'])
        df = df.sort_values(['city_id', 'logged_at'])
        
        # 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        unique_cities = df['city_id'].nunique()
        date_range = f"{df['logged_at'].min()} ‚Üí {df['logged_at'].max()}"
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   üèôÔ∏è –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –º—ñ—Å—Ç: {unique_cities:,}")
        print(f"   üìÖ –ü–µ—Ä—ñ–æ–¥: {date_range}")
        print(f"   üìà –°–µ—Ä–µ–¥–Ω—å–æ –∑–∞–ø–∏—Å—ñ–≤ –Ω–∞ –º—ñ—Å—Ç–æ: {len(df) / unique_cities:.0f}")
        
        # 4. –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ –º—ñ—Å—Ç–∞ –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∑–∞–ø–∏—Å—ñ–≤
        print(f"\nüîç –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –º—ñ—Å—Ç (–º—ñ–Ω—ñ–º—É–º {min_records_per_city} –∑–∞–ø–∏—Å—ñ–≤)...")
        
        city_counts = df['city_id'].value_counts()
        valid_cities = city_counts[city_counts >= min_records_per_city].index.tolist()
        
        df_filtered = df[df['city_id'].isin(valid_cities)].copy()
        
        print(f"   ‚úÖ –ú—ñ—Å—Ç –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(valid_cities):,}")
        print(f"   ‚úÖ –ó–∞–ø–∏—Å—ñ–≤ –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(df_filtered):,}")
        
        # 5. –í–∏–±—ñ—Ä–∫–∞ –º—ñ—Å—Ç (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
        if sample_size and sample_size < len(valid_cities):
            print(f"\nüé≤ –í–∏–±—ñ—Ä–∫–∞ {sample_size} –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –º—ñ—Å—Ç...")
            sampled_cities = np.random.choice(valid_cities, size=sample_size, replace=False)
            df_filtered = df_filtered[df_filtered['city_id'].isin(sampled_cities)]
            
            print(f"   ‚úÖ –§—ñ–Ω–∞–ª—å–Ω–∏—Ö –º—ñ—Å—Ç: {sample_size}")
            print(f"   ‚úÖ –§—ñ–Ω–∞–ª—å–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {len(df_filtered):,}")
        
        # 6. –î–æ–¥–∞—Ç–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
        print("\n‚öôÔ∏è –î–æ–¥–∞–≤–∞–Ω–Ω—è –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤...")
        
        # PM10 —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑ PM2.5
        df_filtered['pm10'] = df_filtered['pm25'] * 1.8
        
        # –Ü–Ω—à—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ - —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        df_filtered['no2'] = 25.0
        df_filtered['so2'] = 10.0
        df_filtered['co'] = 500.0
        df_filtered['o3'] = 40.0
        df_filtered['temperature'] = 15.0
        df_filtered['humidity'] = 70
        
        print("   ‚úÖ –î–æ–¥–∞–Ω–æ: pm10, no2, so2, co, o3, temperature, humidity")
        
        return df_filtered
    
    def create_sequences_from_cities(self, df):
        """
        –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –¥–∞–Ω–∏—Ö –∫–æ–∂–Ω–æ–≥–æ –º—ñ—Å—Ç–∞
        
        Args:
            df: DataFrame –∑ –¥–∞–Ω–∏–º–∏ –≤—Å—ñ—Ö –º—ñ—Å—Ç
        
        Returns:
            X, y - –º–∞—Å–∏–≤–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
        """
        print("\nüîÑ –°–¢–í–û–†–ï–ù–ù–Ø –ü–û–°–õ–Ü–î–û–í–ù–û–°–¢–ï–ô")
        print("=" * 70)
        
        feature_columns = ['pm25', 'temperature', 'humidity', 'pm10', 'no2', 'so2', 'co', 'o3']
        output_features = ['pm25', 'pm10', 'no2', 'so2', 'co', 'o3']
        
        all_X = []
        all_y = []
        
        cities = df['city_id'].unique()
        
        print(f"üìç –û–±—Ä–æ–±–∫–∞ {len(cities):,} –º—ñ—Å—Ç...")
        
        for i, city_id in enumerate(cities, 1):
            # –î–∞–Ω—ñ –¥–ª—è –æ–¥–Ω–æ–≥–æ –º—ñ—Å—Ç–∞
            city_data = df[df['city_id'] == city_id].copy()
            city_data = city_data.sort_values('logged_at')
            
            if len(city_data) < self.sequence_length + 1:
                continue
            
            # –í–∏–±—Ä–∞—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏
            city_values = city_data[feature_columns].values
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –º—ñ—Å—Ç–∞ –æ–∫—Ä–µ–º–æ)
            from sklearn.preprocessing import MinMaxScaler
            scaler = MinMaxScaler()
            city_normalized = scaler.fit_transform(city_values)
            
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            for j in range(len(city_normalized) - self.sequence_length):
                X_seq = city_normalized[j:j + self.sequence_length]
                y_target = city_normalized[j + self.sequence_length]
                
                # y - —Ç—ñ–ª—å–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (6 —à—Ç—É–∫)
                # –Ü–Ω–¥–µ–∫—Å–∏ –≤ feature_columns: pm25=0, pm10=3, no2=4, so2=5, co=6, o3=7
                y_values = y_target[[0, 3, 4, 5, 6, 7]]  # pm25, pm10, no2, so2, co, o3
                
                all_X.append(X_seq)
                all_y.append(y_values)
            
            # –ü—Ä–æ–≥—Ä–µ—Å
            if i % 100 == 0:
                print(f"   üìä –û–±—Ä–æ–±–ª–µ–Ω–æ –º—ñ—Å—Ç: {i}/{len(cities)} ({i/len(cities)*100:.1f}%)")
        
        X = np.array(all_X)
        y = np.array(all_y)
        
        print(f"\n‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π: {len(X):,}")
        print(f"   üìê –§–æ—Ä–º–∞ X: {X.shape} (samples, sequence_length, features)")
        print(f"   üìê –§–æ—Ä–º–∞ y: {y.shape} (samples, output_features)")
        
        return X, y
    
    def pretrain_model(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=64):
        """
        –ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        
        Args:
            X_train, y_train: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
            X_val, y_val: –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ
            epochs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
            batch_size: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞
        """
        print("\nüß† –ü–û–ü–ï–†–ï–î–ù–Ñ –ù–ê–í–ß–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü")
        print("=" * 70)
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å (district_id = 0 –¥–ª—è pre-training)
        model = LSTMForecastModel(district_id=0)
        
        # –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É
        input_shape = (X_train.shape[1], X_train.shape[2])
        model.build_model(input_shape)
        
        print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è:")
        print(f"   üî¢ –ï–ø–æ—Ö–∏: {epochs}")
        print(f"   üì¶ Batch size: {batch_size}")
        print(f"   üìä Train samples: {len(X_train):,}")
        print(f"   üìä Val samples: {len(X_val):,}")
        
        # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ y –¥–ª—è multi-output –º–æ–¥–µ–ª—ñ
        y_train_dict = {
            'pm25': y_train[:, 0],
            'pm10': y_train[:, 1],
            'no2': y_train[:, 2],
            'so2': y_train[:, 3],
            'co': y_train[:, 4],
            'o3': y_train[:, 5]
        }
        
        y_val_dict = {
            'pm25': y_val[:, 0],
            'pm10': y_val[:, 1],
            'no2': y_val[:, 2],
            'so2': y_val[:, 3],
            'co': y_val[:, 4],
            'o3': y_val[:, 5]
        }
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        print("\nüöÄ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è...")
        print("   ‚è≥ –¶–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ 10-30 —Ö–≤–∏–ª–∏–Ω...")
        
        history = model.train(
            X_train, y_train_dict,
            X_val, y_val_dict,
            epochs=epochs,
            batch_size=batch_size
        )
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å —è–∫ pretrained
        pretrained_path = os.path.join(Config.MODEL_PATH, 'lstm_pretrained_saveecobot.keras')
        model.model.save(pretrained_path)
        
        print(f"\n‚úÖ Pre-trained –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {pretrained_path}")
        
        return model, history
    
    def run(self, sample_cities=1000, min_records=100, epochs=30):
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø—Ä–æ—Ü–µ—Å –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        
        Args:
            sample_cities: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –º—ñ—Å—Ç –¥–ª—è –≤–∏–±—ñ—Ä–∫–∏ (None = –≤—Å—ñ)
            min_records: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ –Ω–∞ –º—ñ—Å—Ç–æ
            epochs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –Ω–∞–≤—á–∞–Ω–Ω—è
        """
        print("\n" + "=" * 70)
        print("üöÄ –ü–û–ü–ï–†–ï–î–ù–Ñ –ù–ê–í–ß–ê–ù–ù–Ø –ù–ê –î–ê–ù–ò–• SAVEECOBOT")
        print("=" * 70)
        
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
        df = self.load_and_prepare_data(
            sample_size=sample_cities,
            min_records_per_city=min_records
        )
        
        # 2. –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        X, y = self.create_sequences_from_cities(df)
        
        if len(X) == 0:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ!")
            return
        
        # 3. –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –Ω–∞ train/val
        print("\nüìä –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/val...")
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        print(f"   ‚úÖ Train: {len(X_train):,} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π")
        print(f"   ‚úÖ Val: {len(X_val):,} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π")
        
        # 4. –ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è
        model, history = self.pretrain_model(
            X_train, y_train,
            X_val, y_val,
            epochs=epochs,
            batch_size=64
        )
        
        print("\n" + "=" * 70)
        print("‚úÖ –ü–û–ü–ï–†–ï–î–ù–Ñ –ù–ê–í–ß–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û!")
        print("=" * 70)
        
        print("\nüí° –ù–∞—Å—Ç—É–ø–Ω–∏–π –∫—Ä–æ–∫: Fine-tuning –Ω–∞ –¥–∞–Ω–∏—Ö –õ—å–≤–æ–≤–∞")
        print("   –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π: python scripts/finetune_on_lviv.py")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ SaveEcoBot')
    parser.add_argument('--csv', type=str, required=True, help='–®–ª—è—Ö –¥–æ CSV —Ñ–∞–π–ª—É')
    parser.add_argument('--cities', type=int, default=1000, help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –º—ñ—Å—Ç (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 1000)')
    parser.add_argument('--min-records', type=int, default=100, help='–ú—ñ–Ω—ñ–º—É–º –∑–∞–ø–∏—Å—ñ–≤ –Ω–∞ –º—ñ—Å—Ç–æ')
    parser.add_argument('--epochs', type=int, default=30, help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö')
    
    args = parser.parse_args()
    
    pretrainer = SaveEcoBotPretrainer(args.csv)
    pretrainer.run(
        sample_cities=args.cities,
        min_records=args.min_records,
        epochs=args.epochs
    )